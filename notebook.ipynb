{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Major Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Translation\n",
    "\n",
    "Machine Translation (MT) is the task of automatically converting one natural language into another, preserving the meaning of the input text, and producing fluent text in the output language. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using EasyNMT - Easy to use, state-of-the-art Neural Machine Translation\n",
    "- https://pythonrepo.com/repo/UKPLab-EasyNMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11.9k/11.9k [00:00<00:00, 13.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "from easynmt import EasyNMT\n",
    "model = EasyNMT('opus-mt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938k/938k [00:01<00:00, 697kB/s]  \n",
      "Downloading: 100%|██████████| 793k/793k [00:01<00:00, 565kB/s] \n",
      "Downloading: 100%|██████████| 1.02M/1.02M [00:04<00:00, 242kB/s] \n",
      "Downloading: 100%|██████████| 2.00M/2.00M [00:06<00:00, 323kB/s]\n",
      "Downloading: 100%|██████████| 44.0/44.0 [00:00<00:00, 34.0kB/s]\n",
      "Downloading: 100%|██████████| 1.12k/1.12k [00:00<00:00, 695kB/s]\n",
      "Downloading: 100%|██████████| 292M/292M [02:16<00:00, 2.25MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "यह एक वाक्य है हम जर्मन के लिए अनुवाद करना चाहते हैं\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 750k/750k [00:03<00:00, 225kB/s]  \n",
      "Downloading: 100%|██████████| 778k/778k [00:03<00:00, 218kB/s]  \n",
      "Downloading: 100%|██████████| 1.21M/1.21M [00:08<00:00, 150kB/s] \n",
      "Downloading: 100%|██████████| 42.0/42.0 [00:00<00:00, 28.7kB/s]\n",
      "Downloading: 100%|██████████| 1.30k/1.30k [00:00<00:00, 1.24MB/s]\n",
      "Downloading: 100%|██████████| 284M/284M [01:44<00:00, 2.85MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sie können eine Liste mit Sätzen definieren.', 'Alle Sätze werden in Ihre Zielsprache übersetzt.', 'Beachten Sie, Sie können auch die Sprachen der Sätze mischen.']\n"
     ]
    }
   ],
   "source": [
    "#Translate a single sentence to Hindi\n",
    "print(model.translate('This is a sentence we want to translate to German', target_lang='hi'))\n",
    "\n",
    "#Translate several sentences to German\n",
    "sentences = ['You can define a list with sentences.',\n",
    "             'All sentences are translated to your target language.',\n",
    "             'Note, you could also mix the languages of the sentences.']\n",
    "print(model.translate(sentences, target_lang='de'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "मेरा नाम एन्मर है\n"
     ]
    }
   ],
   "source": [
    "print(model.translate(\"My name is Indra\",target_lang='hi'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  musk born june 28 1971 is an entrepreneur and\n",
      "elon reeve musk frs iln eelon born june 28 1971 is an entrepreneur and business magnate\n",
      "musk cofounded online bank xcom that same year which merged with confinity in 2000 to form paypal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/indrap24/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importing the librtaries\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim import corpora\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "import heapq\n",
    "# text from wikipedia about Elon Musk\n",
    "txt = \"Elon Reeve Musk FRS (/ˈiːlɒn/ EE-lon; born June 28, 1971) is an entrepreneur and business magnate. He is the founder, CEO, and Chief Engineer at SpaceX; early stage investor,[note 1] CEO, and Product Architect of Tesla, Inc.; founder of The Boring Company; and co-founder of Neuralink and OpenAI. A centibillionaire, Musk is one of the richest people in the world.Musk was born to a Canadian mother and South African father and raised in Pretoria, South Africa. He briefly attended the University of Pretoria before moving to Canada aged 17 to attend Queen's University. He transferred to the University of Pennsylvania two years later, where he received bachelor's degrees in economics and physics. He moved to California in 1995 to attend Stanford University but decided instead to pursue a business career, co-founding the web software company Zip2 with his brother Kimbal. The startup was acquired by Compaq for $307 million in 1999. Musk co-founded online bank X.com that same year, which merged with Confinity in 2000 to form PayPal. The company was bought by eBay in 2002 for $1.5 billion.In 2002, Musk founded SpaceX, an aerospace manufacturer and space transport services company, of which he is CEO and CTO. In 2004, he joined electric vehicle manufacturer Tesla Motors, Inc. (now Tesla, Inc.) as chairman and product architect, becoming its CEO in 2008. In 2006, he helped create SolarCity, a solar energy services company that was later acquired by Tesla and became Tesla Energy. In 2015, he co-founded OpenAI, a nonprofit research company that promotes friendly artificial intelligence. In 2016, he co-founded Neuralink, a neurotechnology company focused on developing brain–computer interfaces, and founded The Boring Company, a tunnel construction company. Musk has proposed the Hyperloop, a high-speed vactrain transportation system.Musk has been the subject of criticism due to unorthodox or unscientific stances and highly publicized controversies. In 2018, he was sued for defamation by a diver who advised in the Tham Luang cave rescue; a California jury ruled in favor of Musk. In the same year, he was sued by the US Securities and Exchange Commission (SEC) for falsely tweeting that he had secured funding for a private takeover of Tesla. He settled with the SEC, temporarily stepping down from his chairmanship and accepting limitations on his Twitter usage. Musk has spread misinformation about the COVID-19 pandemic and has received criticism from experts for his other views on such matters as artificial intelligence and public transport.\"\n",
    "\n",
    "#class for preprocessing and creating word embedding\n",
    "class Preprocessing:\n",
    "    #constructor\n",
    "    def __init__(self,txt):\n",
    "        # Tokenization\n",
    "        nltk.download('punkt')  #punkt is nltk tokenizer \n",
    "        # breaking text to sentences\n",
    "        tokens = nltk.sent_tokenize(txt) \n",
    "        self.tokens = tokens\n",
    "        self.tfidfvectoriser=TfidfVectorizer()\n",
    "\n",
    "    # Data Cleaning\n",
    "    # remove extra spaces\n",
    "    # convert sentences to lower case \n",
    "    # remove stopword\n",
    "    def clean_sentence(self, sentence, stopwords=False):\n",
    "        sentence = sentence.lower().strip()\n",
    "        sentence = re.sub(r'[^a-z0-9\\s]', '', sentence)\n",
    "        if stopwords:\n",
    "          sentence = remove_stopwords(sentence)\n",
    "        return sentence\n",
    "\n",
    "    # store cleaned sentences to cleaned_sentences\n",
    "    def get_cleaned_sentences(self,tokens, stopwords=False):\n",
    "        cleaned_sentences = []\n",
    "        for line in tokens:\n",
    "          cleaned = self.clean_sentence(line, stopwords)\n",
    "          cleaned_sentences.append(cleaned)\n",
    "        return cleaned_sentences\n",
    "\n",
    "    #do all the cleaning\n",
    "    def cleanall(self):\n",
    "        cleaned_sentences = self.get_cleaned_sentences(self.tokens, stopwords=True)\n",
    "        cleaned_sentences_with_stopwords = self.get_cleaned_sentences(self.tokens, stopwords=False)\n",
    "        # print(cleaned_sentences)\n",
    "        # print(cleaned_sentences_with_stopwords)\n",
    "        return [cleaned_sentences,cleaned_sentences_with_stopwords]\n",
    "\n",
    "    # TF-IDF Vectorizer\n",
    "    def TFIDF(self,cleaned_sentences):\n",
    "        self.tfidfvectoriser.fit(cleaned_sentences)\n",
    "        tfidf_vectors=self.tfidfvectoriser.transform(cleaned_sentences)\n",
    "        return tfidf_vectors\n",
    "\n",
    "    #tfidf for question\n",
    "    def TFIDF_Q(self,question_to_be_cleaned):\n",
    "        tfidf_vectors=self.tfidfvectoriser.transform([question_to_be_cleaned])\n",
    "        return tfidf_vectors\n",
    "\n",
    "    # main call function\n",
    "    def doall(self):\n",
    "        cleaned_sentences, cleaned_sentences_with_stopwords = self.cleanall()\n",
    "        tfidf = self.TFIDF(cleaned_sentences)\n",
    "        return [cleaned_sentences,cleaned_sentences_with_stopwords,tfidf]\n",
    "\n",
    "#class for answering the question.\n",
    "class AnswerMe:\n",
    "    #cosine similarity\n",
    "    def Cosine(self, question_vector, sentence_vector):\n",
    "        dot_product = np.dot(question_vector, sentence_vector.T)\n",
    "        denominator = (np.linalg.norm(question_vector) * np.linalg.norm(sentence_vector))\n",
    "        return dot_product/denominator\n",
    "    \n",
    "    #Euclidean distance\n",
    "    def Euclidean(self, question_vector, sentence_vector):\n",
    "        vec1 = question_vector.copy()\n",
    "        vec2 = sentence_vector.copy()\n",
    "        if len(vec1)<len(vec2): vec1,vec2 = vec2,vec1\n",
    "        vec2 = np.resize(vec2,(vec1.shape[0],vec1.shape[1]))\n",
    "        return np.linalg.norm(vec1-vec2)\n",
    "\n",
    "    # main call function\n",
    "    def answer(self, question_vector, sentence_vector, method):\n",
    "        if method==1: return self.Euclidean(question_vector,sentence_vector)\n",
    "        else: return self.Cosine(question_vector,sentence_vector)\n",
    "\n",
    "\n",
    "def RetrieveAnswer(question_embedding, tfidf_vectors,method=1):\n",
    "  similarity_heap = []\n",
    "  if method==1: max_similarity = float('inf')\n",
    "  else: max_similarity = -1\n",
    "  index_similarity = -1\n",
    "\n",
    "  for index, embedding in enumerate(tfidf_vectors):  \n",
    "    find_similarity = AnswerMe()\n",
    "    similarity = find_similarity.answer((question_embedding).toarray(),(embedding).toarray() , method).mean()\n",
    "    if method==1:\n",
    "      heapq.heappush(similarity_heap,(similarity,index))\n",
    "    else:\n",
    "      heapq.heappush(similarity_heap,(-similarity,index))\n",
    "  return similarity_heap\n",
    "\n",
    "\n",
    "# Put Your question here\n",
    "user_question = \"musk born june 28 1971 is an entrepreneur and\"\n",
    "#define method\n",
    "method = 2\n",
    "\n",
    "preprocess = Preprocessing(txt)\n",
    "cleaned_sentences,cleaned_sentences_with_stopwords,tfidf_vectors = preprocess.doall()\n",
    "\n",
    "question = preprocess.clean_sentence(user_question, stopwords=True)\n",
    "question_embedding = preprocess.TFIDF_Q(question)\n",
    "\n",
    "similarity_heap = RetrieveAnswer(question_embedding , tfidf_vectors ,method)\n",
    "print(\"Question: \", user_question)\n",
    "\n",
    "number_of_sentences_to_print = 2\n",
    "while number_of_sentences_to_print>0 and len(similarity_heap)>0:\n",
    "  x = similarity_heap.pop(0)\n",
    "  print(cleaned_sentences_with_stopwords[x[1]])\n",
    "  number_of_sentences_to_print-=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "df = pd.read_csv('./Movie Rating Prdiction/Train/Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mature intelligent and highly charged melodram...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://video.google.com/videoplay?docid=211772...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Title: Opera (1987) Director: Dario Argento Ca...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I think a lot of people just wrote this off as...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is a story of two dogs and a cat looking ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Steve Carell comes into his own in his first s...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I'm only going to write more because it's requ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>OK, it was a \"risky\" move to rent this flick, ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Cannibalism, a pair of cinematic references to...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>This is one of the great modern kung fu films....</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Steve Martin looks like he's had a face lift. ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Robert Jordan is a television star. Robert Jor...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Probably the best comedy in a long time. keeps...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The earlier review is pretty much on target, w...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>It is depressing that many people don't unders...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>This, like Murder She Wrote, is one of those s...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>This film is a sleeper because Rod Steiger's i...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Give director Stanley Tong of Jackie Chan's Su...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>This is another of my favorite Columbos. It sp...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>This film IS brilliant...... without a doubt. ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review label\n",
       "0   mature intelligent and highly charged melodram...   pos\n",
       "1   http://video.google.com/videoplay?docid=211772...   pos\n",
       "2   Title: Opera (1987) Director: Dario Argento Ca...   pos\n",
       "3   I think a lot of people just wrote this off as...   pos\n",
       "4   This is a story of two dogs and a cat looking ...   pos\n",
       "5   Steve Carell comes into his own in his first s...   pos\n",
       "6   I'm only going to write more because it's requ...   neg\n",
       "7   OK, it was a \"risky\" move to rent this flick, ...   neg\n",
       "8   Cannibalism, a pair of cinematic references to...   pos\n",
       "9   This is one of the great modern kung fu films....   pos\n",
       "10  Steve Martin looks like he's had a face lift. ...   neg\n",
       "11  Robert Jordan is a television star. Robert Jor...   pos\n",
       "12  Probably the best comedy in a long time. keeps...   pos\n",
       "13  The earlier review is pretty much on target, w...   neg\n",
       "14  It is depressing that many people don't unders...   pos\n",
       "15  This, like Murder She Wrote, is one of those s...   pos\n",
       "16  This film is a sleeper because Rod Steiger's i...   pos\n",
       "17  Give director Stanley Tong of Jackie Chan's Su...   neg\n",
       "18  This is another of my favorite Columbos. It sp...   pos\n",
       "19  This film IS brilliant...... without a doubt. ...   pos"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = df.iloc[:,0].values\n",
    "dfy = df.iloc[:,1].values\n",
    "temp = [x for x in dfy]\n",
    "y_train = [1 if x == 'pos' else 0 for x in temp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanin Training data\n",
    "sw = stopwords.words('english')\n",
    "sw.remove('not')\n",
    "sw = set(sw)\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def cleaning_pipeline(review):\n",
    "    words = word_tokenize(review.lower())\n",
    "    words = [ps.stem(word) for word in words if word not in sw and word.isalpha()]\n",
    "    review = \" \".join(words)\n",
    "    return review\n",
    "\n",
    "cleaned_reviews = [ cleaning_pipeline(review) for review in dfx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and cleaning testing data\n",
    "dftest = pd.read_csv('./Movie Rating Prdiction/Test/Test.csv').values\n",
    "test_reviews = dftest.reshape((-1,))\n",
    "cleaned_test_rev = [cleaning_pipeline(review) for review in test_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vectorization\n",
    "cv = CountVectorizer(ngram_range=(1,3))\n",
    "x_train_vect = cv.fit_transform(cleaned_reviews)\n",
    "x_test_vect = cv.transform(cleaned_test_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial NB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Trainig\n",
    "mnb.fit(x_train_vect,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction\n",
    "prediction = mnb.predict(x_test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99975"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#score\n",
    "mnb.score(x_train_vect,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There are many techniques available to generate extractive summarization to keep it simple, I will be using an unsupervised learning approach to find the sentences similarity and rank them. Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them.\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Input text - to summarize\n",
    "text = \"\"\"There are many techniques available to generate extractive summarization to keep it simple, I will be using an unsupervised learning approach to find the sentences similarity and rank them. Summarization can be defined as a task of producing a concise and fluent summary while preserving key information and overall meaning. One benefit of this will be, you don’t need to train and build a model prior start using it for your project. It’s good to understand Cosine similarity to make the best use of the code you are going to see. Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. Its measures cosine of the angle between vectors. The angle will be 0 if sentences are similar.\"\"\"\n",
    "\n",
    "# Tokenizing the text\n",
    "stopWords = set(stopwords.words(\"english\"))\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Creating a frequency table to keep the\n",
    "# score of each word\n",
    "\n",
    "freqTable = dict()\n",
    "for word in words:\n",
    "\tword = word.lower()\n",
    "\tif word in stopWords:\n",
    "\t\tcontinue\n",
    "\tif word in freqTable:\n",
    "\t\tfreqTable[word] += 1\n",
    "\telse:\n",
    "\t\tfreqTable[word] = 1\n",
    "\n",
    "# Creating a dictionary to keep the score\n",
    "# of each sentence\n",
    "sentences = sent_tokenize(text)\n",
    "sentenceValue = dict()\n",
    "\n",
    "for sentence in sentences:\n",
    "\tfor word, freq in freqTable.items():\n",
    "\t\tif word in sentence.lower():\n",
    "\t\t\tif sentence in sentenceValue:\n",
    "\t\t\t\tsentenceValue[sentence] += freq\n",
    "\t\t\telse:\n",
    "\t\t\t\tsentenceValue[sentence] = freq\n",
    "\n",
    "\n",
    "\n",
    "sumValues = 0\n",
    "for sentence in sentenceValue:\n",
    "\tsumValues += sentenceValue[sentence]\n",
    "\n",
    "# Average value of a sentence from the original text\n",
    "\n",
    "average = int(sumValues / len(sentenceValue))\n",
    "\n",
    "# Storing sentences into our summary.\n",
    "summary = ''\n",
    "for sentence in sentences:\n",
    "\tif (sentence in sentenceValue) and (sentenceValue[sentence] > (1.2 * average)):\n",
    "\t\tsummary += \" \" + sentence\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spam Filtering\n",
    "\n",
    "Automated Spam E-mail Detection Using common NLP tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/indrap24/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/indrap24/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/indrap24/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/indrap24/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wordcloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>job posting - apple-iss research center</td>\n",
       "      <td>content - length : 3386 apple-iss research cen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>lang classification grimes , joseph e . and ba...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>query : letter frequencies for text identifica...</td>\n",
       "      <td>i am posting this inquiry for sergei atamas ( ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>risk</td>\n",
       "      <td>a colleague and i are researching the differin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>request book information</td>\n",
       "      <td>earlier this morning i was on the phone with a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             subject  \\\n",
       "0            job posting - apple-iss research center   \n",
       "1                                                NaN   \n",
       "2  query : letter frequencies for text identifica...   \n",
       "3                                               risk   \n",
       "4                           request book information   \n",
       "\n",
       "                                             message  label  \n",
       "0  content - length : 3386 apple-iss research cen...      0  \n",
       "1  lang classification grimes , joseph e . and ba...      0  \n",
       "2  i am posting this inquiry for sergei atamas ( ...      0  \n",
       "3  a colleague and i are researching the differin...      0  \n",
       "4  earlier this morning i was on the phone with a...      0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"messages.csv\",encoding='latin-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2893 entries, 0 to 2892\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   subject  2831 non-null   object\n",
      " 1   message  2893 non-null   object\n",
      " 2   label    2893 non-null   int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 67.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2412\n",
       "1     481\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see that 1 stand for Spam mail and 0 stand for not a spam mail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not a Spam Email Ratio i.e. 0 label: 83.0 %\n",
      "Spam Email Ratio that is 1 label: 17.0 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Not a Spam Email Ratio i.e. 0 label:\",round(len(df[df['label']==0])/len(df['label']),2)*100,\"%\")\n",
    "print(\"Spam Email Ratio that is 1 label:\",round(len(df[df['label']==1])/len(df['label']),2)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>job posting - apple-iss research center</td>\n",
       "      <td>content - length : 3386 apple-iss research cen...</td>\n",
       "      <td>0</td>\n",
       "      <td>2856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>lang classification grimes , joseph e . and ba...</td>\n",
       "      <td>0</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>query : letter frequencies for text identifica...</td>\n",
       "      <td>i am posting this inquiry for sergei atamas ( ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>risk</td>\n",
       "      <td>a colleague and i are researching the differin...</td>\n",
       "      <td>0</td>\n",
       "      <td>324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>request book information</td>\n",
       "      <td>earlier this morning i was on the phone with a...</td>\n",
       "      <td>0</td>\n",
       "      <td>1046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             subject  \\\n",
       "0            job posting - apple-iss research center   \n",
       "1                                                NaN   \n",
       "2  query : letter frequencies for text identifica...   \n",
       "3                                               risk   \n",
       "4                           request book information   \n",
       "\n",
       "                                             message  label  length  \n",
       "0  content - length : 3386 apple-iss research cen...      0    2856  \n",
       "1  lang classification grimes , joseph e . and ba...      0    1800  \n",
       "2  i am posting this inquiry for sergei atamas ( ...      0    1435  \n",
       "3  a colleague and i are researching the differin...      0     324  \n",
       "4  earlier this morning i was on the phone with a...      0    1046  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['length'] = df.message.str.len()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english') + ['u', 'ü', 'ur', '4', '2', 'im', 'dont', 'doin', 'ure'])\n",
    "\n",
    "df['message'] = df['message'].apply(lambda x: \" \".join(term for term in x.split() if term not in stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vec = TfidfVectorizer()\n",
    "#naive = MultinomialNB()\n",
    "SVM = SVC(C=1.0, kernel='linear', degree=3 , gamma='auto')\n",
    "features = tf_vec.fit_transform(df['message'])\n",
    "X = features\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.987434554973822"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = SVM.fit(X_train, y_train).predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       773\n",
      "           1       0.99      0.94      0.97       182\n",
      "\n",
      "    accuracy                           0.99       955\n",
      "   macro avg       0.99      0.97      0.98       955\n",
      "weighted avg       0.99      0.99      0.99       955\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech Recognition\n",
    "\n",
    "Speech is the most common means of communication and the majority of the population in the world relies on speech to communicate with one another. Speech recognition system basically translates spoken languages into text. There are various real-life examples of speech recognition systems. For example, Apple SIRI which recognize the speech and truncates into text.\n",
    "\n",
    "\n",
    "Hidden Markov Model (HMM), deep neural network models are used to convert the audio into text.  This can be done with the help of the `Speech Recognition API`(*Google speech recognition API*) and `PyAudio` library.\n",
    "\n",
    "\n",
    "Steps:\n",
    "- Import Speech recognition library\n",
    "- Initializing recognizer class in order to recognize the speech. We are using google speech recognition.\n",
    "- Audio file supports by speech recognition: wav, AIFF, AIFF-C, FLAC. I used `wav` file in this example\n",
    "- I have used a sample audio clip which says “**Hello, my name is Shobhit.**”\n",
    "- By default, google recognizer reads English. It supports different languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Audio.wav file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting audio transcripts into text ...\n",
      "hello my name is\n"
     ]
    }
   ],
   "source": [
    "#import library\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Initialize recognizer class (for recognizing the speech)\n",
    "r = sr.Recognizer()\n",
    "\n",
    "# Reading Audio file as source\n",
    "# listening the audio file and store in audio_text variable\n",
    "\n",
    "with sr.AudioFile('./sampleAudio.wav') as source:\n",
    "    \n",
    "    audio_text = r.listen(source)\n",
    "    \n",
    "# recoginize_() method will throw a request error if the API is unreachable, hence using exception handling\n",
    "    try:\n",
    "        \n",
    "        # using google speech recognition\n",
    "        text = r.recognize_google(audio_text)\n",
    "        print('Converting audio transcripts into text ...')\n",
    "        print(text)\n",
    "     \n",
    "    except:\n",
    "         print('Sorry.. run again...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the API cannot recognize the name **Shobhit** since its not an English name, but can recognize the other words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answering Pipeline\n",
    "\n",
    "QA systems can be described as a technology that provides the right short answer to a question rather than giving a list of possible answers. In this scenario, QA systems are designed to be alert to text similarity and answer questions that are asked in natural language.\n",
    "\n",
    "The Transformer architecture which has become a state-of-the-art approach in text based models since 2017, many Machine Learning tasks involving language can now be performed with unprecedented results. Question answering is one such task for which Machine Learning can be used. Here, we will build a Question Answering model pipeline in a really easy way using the HuggingFace Transformers library. This library is becoming increasingly important for democratizing Transformer based approaches in Machine Learning and allows people to use Transformers out-of-the-box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad (https://huggingface.co/distilbert-base-cased-distilled-squad)\n",
      "Downloading: 100%|██████████| 473/473 [00:00<00:00, 92.0kB/s]\n",
      "Downloading: 100%|██████████| 249M/249M [01:30<00:00, 2.88MB/s] \n",
      "Downloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 13.6kB/s]\n",
      "Downloading: 100%|██████████| 208k/208k [00:01<00:00, 158kB/s]  \n",
      "Downloading: 100%|██████████| 426k/426k [00:02<00:00, 176kB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the capital of the Netherlands?\n",
      "Answer: 'Amsterdam' with score 0.37749919295310974\n"
     ]
    }
   ],
   "source": [
    "# Open and read the article\n",
    "question = \"What is the capital of the Netherlands?\"\n",
    "context = r\"The four largest cities in the Netherlands are Amsterdam, Rotterdam, The Hague and Utrecht.[17] Amsterdam is the country's most populous city and nominal capital,[18] while The Hague holds the seat of the States General, Cabinet and Supreme Court.[19] The Port of Rotterdam is the busiest seaport in Europe, and the busiest in any country outside East Asia and Southeast Asia, behind only China and Singapore.\"\n",
    "\n",
    "# Generating an answer to the question in context\n",
    "qa = pipeline(\"question-answering\")\n",
    "answer = qa(question=question, context=context)\n",
    "\n",
    "# Print the answer\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: '{answer['answer']}' with score {answer['score']}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "68321d387b255d8a332b80d9cc4ccf80ca2f7bc46b6a2ad5a25ae75181723480"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('nlpAssnmt3.8': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
